{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's explore caching and profiling.\n",
    "\n",
    "Important words: \n",
    "\n",
    "**caching** - Saving a piece of information, usually one that is computationally expensive to fetch, in fast-access memory to hand out to clients who ask for that piece of information. Usually done after the first time the piece of info is fetched. When the result of a function is cached for a specific set of arguments to the function, it's called **memoization**.\n",
    "\n",
    "**profiling** - Running our code with a tool that tells us how much time and/or memory each portion of our code is using. It is (or should be) the first step in any serious code optimization effort. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In REAL LIFE the module you'll import this from is called functools.\n",
    "# In this version, I deliberately patched lru_cache to give you access to the cache,\n",
    "# Which you don't get in the \"real\" version for thread safety reasons.\n",
    "from chelseas_functools import lru_cache \n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I have written a recursive function that calculates the _factorial_ of a number.\n",
    "\n",
    "Example: 5 factorial, or 5!, is 5 * 4 * 3 * 2 * 1, or 120.\n",
    "\n",
    "I have decorated it with the `@lru_cache` decorator with a `maxsize` argument of `None`.\n",
    "\n",
    "[Theis documentation](https://docs.python.org/3/library/functools.html) covers how `@lru_cache` works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def factorial(num):\n",
    "    if num == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return num * factorial(num-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I run my factorial function on the number 50 with Python's profiler. [Here is the documentation](https://docs.python.org/3/library/profile.html) on how to read profiler output.\n",
    "\n",
    "Run the cell below a couple of times and notice how the profiler output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('factorial(51)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@lru_cache` decorator adds two methods to the `factorial` function:\n",
    "    \n",
    "1. `cache_info`, which gives you four pieces of information about the cache (what are they?)\n",
    "1. `cache_clear`, which clears the cache so you can run the function \"from scratch\" again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorial.cache_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK, so here's where I did something naughty.\n",
    "\n",
    "I augmented the `@lru_cache` wrapper to give you access to the cache. DO NOT USE the `chelseas_functools` version of `@lru_cache` in any production application. But for our _educational_ purposes, you can here peek inside the cache that the function is keeping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorial.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many items are in this cache? Is that how many items you expected this cache to have in there?\n",
    "\n",
    "Does this cache **remind** you of any in-class exercises we have done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try out @lru_cache with a maxsize.\n",
    "\n",
    "So far in this exercise, we have had the `@lru_cache`'s `maxsize` argument set to `None`. This means that the cache increases in size _indefinitely_. Python has another decorator called `@cache` that has the same behavior as `@lru_cache` with a `maxsize` set to `None`.\n",
    "\n",
    "#### Questions for you:\n",
    "\n",
    "1. Based on the documentation you read, what is the default value for `maxsize` in `@lru_cache`?\n",
    "2. What are the units of `maxsize`? What is it saving that many of?\n",
    "\n",
    "Let's set the `@lru_cache`'s `maxsize` to something else: ten, for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=10)\n",
    "def factorial(num):\n",
    "    if num == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return num * factorial(num-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run our wrapped function on a number (I have deliberately chosen a small number to make the following examples easier to visually inspect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorial(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here's where things get wild**: observe below how the data structure used for the cache _changes_ when the cache has a `maxsize`. \n",
    "\n",
    "What data structure is that?\n",
    "Why do you think it's getting used here? \n",
    "\n",
    "**Hint #1:** When `@lru_cache` has a limited size, how does it decide which items to keep and which items to get rid of? Does the data structure we saw above support that use case?\n",
    "\n",
    "**Hint #2:** you can go _look_ at the actual, _real_ implementation of `lru_cache` [right here](https://github.com/python/cpython/blob/main/Lib/functools.py)â€”or even my copied version in the `chelseas_functools` module in this directory, which is _almost exactly_ the same as the real implementation. Search in the file for `def _lru_cache_wrapper`, which (shocking!) defines the `lru_cache` wrapper. See how that function _switches_ how it implements the cache based on the `maxsize` argument?\n",
    "\n",
    "Here's what the cache looks like when it has a `maxsize`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorial.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id(factorial.cache[1][1]) == id(factorial.cache[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
